# train a miniature character-level shakespeare model
# good for debugging and playing on macbooks and such

dataset: 
    dataset_dir: tests/unit/data

model:
    vocab_size: 65 # characters only
    n_block: 16
    n_layer: 1
    n_heads: 2
    n_embed: 32
    dropout: 0.2

training:
    eval_interval: 10 # keep frequent because we'll overfit
    eval_iters: 25
    log_interval: 10 # don't print too too often
    always_save_checkpoint: false  # only save when val improves
    wandb_log: false # override via command line if you like
    n_batch: 64
    learning_rate: 1.0e-3 # with baby networks can afford to go a bit higher
    max_iters: 100
    lr_decay_iters: 100 # make equal to max_iters usually
    min_lr: 1.0e-4 # learning_rate / 10 usually
    beta2: 0.99 # make a bit bigger because number of tokens per iter is small
    warmup_iters: 10 # not super necessary potentially
    # on macbook also add
    # device = 'cpu'  # run on cpu only
    torch_compile: false # do not torch compile the model

checkpoint:
    checkpoint_dir: tests/unit/out

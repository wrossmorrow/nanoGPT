# train a miniature character-level shakespeare model
# good for debugging and playing on macbooks and such

dataset: 
    dataset_dir: data/shakespeare/tiny/char

model:
    vocab_size: 65 # characters only
    n_block: 256
    n_layer: 1
    n_heads: 1
    n_embed: 14
    # n_qkdim: 2
    dropout: 0.0
    attention_only: true
    batched_qkv: false
    linear_layernorms: false
    attn_bias: false
    attn_dropout: 0.0

training:
    eval_interval: 10 # keep frequent because we'll overfit
    eval_iters: 100
    log_interval: 10 # don't print too too often
    always_save_checkpoint: false  # only save when val improves
    wandb_log: false # override via command line if you like
    n_batch: 10
    learning_rate: 1.0e-3 # with baby networks can afford to go a bit higher
    max_iters: 500
    grad_clip: 0.0
    lr_decay_iters: 5000 # make equal to max_iters usually
    min_lr: 1.0e-4 # learning_rate / 10 usually
    beta2: 0.99 # make a bit bigger because number of tokens per iter is small
    warmup_iters: 100 # not super necessary potentially
    # on macbook also add
    # device = 'cpu'  # run on cpu only
    torch_compile: false # do not torch compile the model

checkpoint:
    checkpoint_dir: out/shakespeare/tiny/char

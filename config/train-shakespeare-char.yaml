# train a miniature character-level shakespeare model
# good for debugging and playing on macbooks and such

dataset: 
    dataset_dir: data/shakespeare-char

model:
    vocab_size: 65 # characters only
    n_block: 256 # context of up to 256 previous characters
    n_layer: 6
    n_heads: 6
    n_embed: 384
    dropout: 0.2

training:
    eval_interval: 250 # keep frequent because we'll overfit
    eval_iters: 200
    log_interval: 10 # don't print too too often

    # we expect to overfit on this small dataset, so only save when val improves
    always_save_checkpoint: false

    wandb_log: false # override via command line if you like

    # dataset: shakespeare_char
    n_batch: 64

    learning_rate: 1.0e-3 # with baby networks can afford to go a bit higher
    max_iters: 5000
    lr_decay_iters: 5000 # make equal to max_iters usually
    min_lr: 1.0e-4 # learning_rate / 10 usually
    beta2: 0.99 # make a bit bigger because number of tokens per iter is small

    warmup_iters: 100 # not super necessary potentially

    # on macbook also add
    # device = 'cpu'  # run on cpu only
    torch_compile: false # do not torch compile the model

checkpoint:
    checkpoint_dir: out/shakespeare-char/full
